{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a507b903-40ec-4b17-a08a-b95994b95181",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1206cd4a-eac2-47e0-b93e-6fabad2e0165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghavsikaria/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "from notebookjs import execute_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee15dbf0-ea97-4a77-806d-57a5bb82c3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting SPACY configs\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "nlp_spacy.max_length = 1500000\n",
    "\n",
    "# Setting NLTK configs\n",
    "stop_words = stopwords.words('english')\n",
    "ps = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af65aa-eab6-44c3-b6c0-b691879b276d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Initializing D3JS source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e931f032-19ca-4e86-b0a0-de3f6064f25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d3_path = \"https://d3js.org/d3.v7.min.js\"\n",
    "code_vis_box_plot  = open(\"d3js_javascript_files/vis_box_plot.js\", \"r\").read()\n",
    "code_vis_bubble_chart  = open(\"d3js_javascript_files/vis_bubble_chart.js\", \"r\").read()\n",
    "code_vis_grouped_bar_chart  = open(\"d3js_javascript_files/vis_grouped_bar_chart.js\", \"r\").read()\n",
    "code_vis_pie_charts  = open(\"d3js_javascript_files/vis_pie_charts.js\", \"r\").read()\n",
    "code_vis_simple_bar_charts  = open(\"d3js_javascript_files/vis_simple_bar_charts.js\", \"r\").read()\n",
    "code_vis_small_multiple_area_chart  = open(\"d3js_javascript_files/vis_small_multiple_area_chart.js\", \"r\").read()\n",
    "code_viz_lda_topics_matrix = open(\"d3js_javascript_files/viz_lda_topics_matrix.js\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2268100-8ba0-4b59-b6b1-83819454d095",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Initializing FINBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080c3ec0-4510-4e17-92cd-586305f820fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "nlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1b10a-01f3-4afa-870e-a95705ee68c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Reading Earnings Calls Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79817033-63cc-4211-aa13-a5b46dd0e52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_data_list = []\n",
    "path = \"/Users/raghavsikaria/Sangharsh/NYU Sem 2/VML/project/transcript_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d17999e-9aba-4d12-aa0d-ed545e8458ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: wfc.txt\n",
      "filename: gs.txt\n",
      "filename: usb.txt\n",
      "filename: jpm.txt\n",
      "filename: cof.txt\n",
      "filename: ms.txt\n",
      "filename: tfc.txt\n",
      "filename: td.txt\n",
      "filename: c.txt\n",
      "filename: bac.txt\n",
      "filename: pnc.txt\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(path):\n",
    "    filepath = os.path.join(path, filename)\n",
    "    print(f\"filename: {filename}\")\n",
    "    \n",
    "    with open(filepath, mode='r') as f:\n",
    "        content = f.read()\n",
    "        transcript_data_list.append({\"company\": filename.split(\".\")[0], \"transcript\": content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3187998-2554-4d88-bccd-23413ee4fe5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wfc</td>\n",
       "      <td>John Campbell\\n\\nGood morning. Thank you for j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs</td>\n",
       "      <td>Carey Halio\\n\\nGood morning. This is Carey Hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usb</td>\n",
       "      <td>George Anderson\\n\\nThank you, Brad. Good morni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jpm</td>\n",
       "      <td>Jeremy Barnum\\n\\nThanks, and good morning, eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cof</td>\n",
       "      <td>Jeff Norris\\n\\nThanks very much, Amy, and welc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ms</td>\n",
       "      <td>James Gorman\\n\\nGood morning, everyone and tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfc</td>\n",
       "      <td>Ankur Vyas\\n\\nThank you, Allay, and good morni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>td</td>\n",
       "      <td>Brooke Hales\\n\\nThank you, Operator. Good afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c</td>\n",
       "      <td>Jen Landis\\n\\nThank you, operator. Good mornin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bac</td>\n",
       "      <td>Lee McEntire\\n\\nThank you, Catherine. Good mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pnc</td>\n",
       "      <td>William Demchak\\n\\nThank you, Bryan, and good ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company                                         transcript\n",
       "0      wfc  John Campbell\\n\\nGood morning. Thank you for j...\n",
       "1       gs  Carey Halio\\n\\nGood morning. This is Carey Hal...\n",
       "2      usb  George Anderson\\n\\nThank you, Brad. Good morni...\n",
       "3      jpm  Jeremy Barnum\\n\\nThanks, and good morning, eve...\n",
       "4      cof  Jeff Norris\\n\\nThanks very much, Amy, and welc...\n",
       "5       ms  James Gorman\\n\\nGood morning, everyone and tha...\n",
       "6      tfc  Ankur Vyas\\n\\nThank you, Allay, and good morni...\n",
       "7       td  Brooke Hales\\n\\nThank you, Operator. Good afte...\n",
       "8        c  Jen Landis\\n\\nThank you, operator. Good mornin...\n",
       "9      bac  Lee McEntire\\n\\nThank you, Catherine. Good mor...\n",
       "10     pnc  William Demchak\\n\\nThank you, Bryan, and good ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(transcript_data_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e234f-1b38-40f6-ab32-a6de09674049",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Applying techniques for basic text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd54060f-f6eb-4a8f-acb7-23fa7b8a0c51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>transcript</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wfc</td>\n",
       "      <td>John Campbell\\n\\nGood morning. Thank you for j...</td>\n",
       "      <td>4237</td>\n",
       "      <td>27344</td>\n",
       "      <td>1531</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs</td>\n",
       "      <td>Carey Halio\\n\\nGood morning. This is Carey Hal...</td>\n",
       "      <td>2585</td>\n",
       "      <td>16623</td>\n",
       "      <td>911</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usb</td>\n",
       "      <td>George Anderson\\n\\nThank you, Brad. Good morni...</td>\n",
       "      <td>2341</td>\n",
       "      <td>15185</td>\n",
       "      <td>846</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jpm</td>\n",
       "      <td>Jeremy Barnum\\n\\nThanks, and good morning, eve...</td>\n",
       "      <td>1991</td>\n",
       "      <td>12535</td>\n",
       "      <td>702</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cof</td>\n",
       "      <td>Jeff Norris\\n\\nThanks very much, Amy, and welc...</td>\n",
       "      <td>2424</td>\n",
       "      <td>15581</td>\n",
       "      <td>835</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ms</td>\n",
       "      <td>James Gorman\\n\\nGood morning, everyone and tha...</td>\n",
       "      <td>2137</td>\n",
       "      <td>13983</td>\n",
       "      <td>732</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfc</td>\n",
       "      <td>Ankur Vyas\\n\\nThank you, Allay, and good morni...</td>\n",
       "      <td>4445</td>\n",
       "      <td>28748</td>\n",
       "      <td>1621</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>td</td>\n",
       "      <td>Brooke Hales\\n\\nThank you, Operator. Good afte...</td>\n",
       "      <td>3973</td>\n",
       "      <td>26737</td>\n",
       "      <td>1266</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c</td>\n",
       "      <td>Jen Landis\\n\\nThank you, operator. Good mornin...</td>\n",
       "      <td>4797</td>\n",
       "      <td>29936</td>\n",
       "      <td>1849</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bac</td>\n",
       "      <td>Lee McEntire\\n\\nThank you, Catherine. Good mor...</td>\n",
       "      <td>6242</td>\n",
       "      <td>38407</td>\n",
       "      <td>2393</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pnc</td>\n",
       "      <td>William Demchak\\n\\nThank you, Bryan, and good ...</td>\n",
       "      <td>3109</td>\n",
       "      <td>19430</td>\n",
       "      <td>1165</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company                                         transcript  word_count   \n",
       "0      wfc  John Campbell\\n\\nGood morning. Thank you for j...        4237  \\\n",
       "1       gs  Carey Halio\\n\\nGood morning. This is Carey Hal...        2585   \n",
       "2      usb  George Anderson\\n\\nThank you, Brad. Good morni...        2341   \n",
       "3      jpm  Jeremy Barnum\\n\\nThanks, and good morning, eve...        1991   \n",
       "4      cof  Jeff Norris\\n\\nThanks very much, Amy, and welc...        2424   \n",
       "5       ms  James Gorman\\n\\nGood morning, everyone and tha...        2137   \n",
       "6      tfc  Ankur Vyas\\n\\nThank you, Allay, and good morni...        4445   \n",
       "7       td  Brooke Hales\\n\\nThank you, Operator. Good afte...        3973   \n",
       "8        c  Jen Landis\\n\\nThank you, operator. Good mornin...        4797   \n",
       "9      bac  Lee McEntire\\n\\nThank you, Catherine. Good mor...        6242   \n",
       "10     pnc  William Demchak\\n\\nThank you, Bryan, and good ...        3109   \n",
       "\n",
       "    char_count  stopwords  numerics  \n",
       "0        27344       1531        15  \n",
       "1        16623        911        15  \n",
       "2        15185        846        15  \n",
       "3        12535        702         5  \n",
       "4        15581        835        35  \n",
       "5        13983        732        10  \n",
       "6        28748       1621        41  \n",
       "7        26737       1266        31  \n",
       "8        29936       1849        24  \n",
       "9        38407       2393        45  \n",
       "10       19430       1165        28  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_count'] = df['transcript'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['char_count'] = df['transcript'].str.len()\n",
    "df['stopwords'] = df['transcript'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\n",
    "df['numerics'] = df['transcript'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37720e-c150-46bd-aa0f-2c77096ff9bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing data and using FinBERT to get sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d8ccf51-5d6a-466d-a0f3-1b0e8653df4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_transcript_corpus(raw_transcript_corpus):\n",
    "    ptc = re.sub('[^a-zA-Z]', ' ', raw_transcript_corpus)\n",
    "    ptc = ptc.lower()\n",
    "    ptc = ptc.split()\n",
    "    processed_corpus = [ps.lemmatize(word) for word in ptc if not word in stop_words]\n",
    "    processed_corpus = ' '.join(processed_corpus)\n",
    "    return processed_corpus\n",
    "\n",
    "def top_10_words(processed_corpus):\n",
    "    word_counts = Counter(processed_corpus.split(\" \")) \n",
    "    return word_counts.most_common(10)\n",
    "\n",
    "def tokenize_sentences(raw_corpus):\n",
    "    return sent_tokenize(raw_corpus)\n",
    "\n",
    "def apply_finbert(tokenized_raw_sentences):\n",
    "    return nlp(tokenized_raw_sentences)\n",
    "\n",
    "def sentence_counter(tokenized_raw_sentences):\n",
    "    return len(tokenized_raw_sentences)\n",
    "\n",
    "def score_array_converter(finbert_scores):\n",
    "    label_score_mapper = {'Neutral': 0, 'Positive': 1, 'Negative': -1}\n",
    "    scores = []\n",
    "    for item in finbert_scores:\n",
    "        scores.append(label_score_mapper[item['label']])\n",
    "    return scores\n",
    "\n",
    "def score_array_cumulative(score_array):\n",
    "    prefixed_sum_score = []\n",
    "    prefixed_sum_score.append(score_array[0])\n",
    "    \n",
    "    for i in range(1, len(score_array)):\n",
    "        prefixed_sum_score.append(score_array[i] + prefixed_sum_score[i-1])\n",
    "    return prefixed_sum_score\n",
    "\n",
    "def count_sentiment_totals(finbert_score_array): \n",
    "    sentiment_totals = {1: 0, -1: 0, 0: 0}\n",
    "    for i in finbert_score_array:\n",
    "        sentiment_totals[i] += 1\n",
    "    return sentiment_totals\n",
    "\n",
    "lda_corpus_list = []\n",
    "def process_for_lda(raw_transcript):\n",
    "    # Code source: https://highdemandskills.com/topic-modeling-lda/#h3-2\n",
    "    # all credits to the author\n",
    "\n",
    "    raw_transcript = raw_transcript.strip()  # Remove white space at the beginning and end\n",
    "    raw_transcript = raw_transcript.replace('\\n', ' ') # Replace the \\n (new line) character with space\n",
    "    raw_transcript = raw_transcript.replace('\\r', '') # Replace the \\r (carriage returns -if you're on windows) with null\n",
    "    raw_transcript = raw_transcript.replace(' ', ' ') # Replace \" \" (a special character for space in HTML) with space. \n",
    "    while '  ' in raw_transcript:\n",
    "        raw_transcript = raw_transcript.replace('  ', ' ') \n",
    "        \n",
    "    proc_spacy_transcript = nlp_spacy(raw_transcript)\n",
    "    temp_list = []\n",
    "    \n",
    "    for token in proc_spacy_transcript:\n",
    "        if token.is_stop == False and token.is_punct == False and (token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\" or token.pos_ ==\"VERB\"):\n",
    "            temp_list.append(token.lemma_.lower())\n",
    "    lda_corpus_list.append(temp_list)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649d17c3-3d2e-46ca-9efa-eac43177306e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['p_transcript']=df['transcript'].apply(process_transcript_corpus)\n",
    "df['top_10_words']=df['p_transcript'].apply(top_10_words)\n",
    "df['tokenized_raw_sentences']=df['transcript'].apply(tokenize_sentences)\n",
    "df['finbert_scores']=df['tokenized_raw_sentences'].apply(apply_finbert)\n",
    "df['number_tokenized_raw_sentences']=df['tokenized_raw_sentences'].apply(sentence_counter)\n",
    "df['finbert_score_array']=df['finbert_scores'].apply(score_array_converter)\n",
    "df['finbert_score_array_cumulative']=df['finbert_score_array'].apply(score_array_cumulative)\n",
    "df['sentiment_totals']=df['finbert_score_array'].apply(count_sentiment_totals)\n",
    "df['processed_corpus_for_lda']=df['transcript'].apply(process_for_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126eab77-ce60-43c7-a4b0-325a67ab3dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "companies_full_name = ['Wells Fargo & Co', 'Goldman Sachs Group Inc', 'US Bancorp', 'JPMorgan Chase & Co', 'Capital One Financial Corp.', 'Morgan Stanley', 'Truist Financial Corp', 'Toronto-Dominion Bank', 'Citigroup Inc', 'Bank of America Corp', 'PNC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9957ede7-d57e-43b2-bf7d-0cde37ab7362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['full_company_name'] = companies_full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc53f98-6fdf-4163-94bb-247fdad28ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>transcript</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>p_transcript</th>\n",
       "      <th>top_10_words</th>\n",
       "      <th>tokenized_raw_sentences</th>\n",
       "      <th>finbert_scores</th>\n",
       "      <th>number_tokenized_raw_sentences</th>\n",
       "      <th>finbert_score_array</th>\n",
       "      <th>finbert_score_array_cumulative</th>\n",
       "      <th>sentiment_totals</th>\n",
       "      <th>processed_corpus_for_lda</th>\n",
       "      <th>full_company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wfc</td>\n",
       "      <td>John Campbell\\n\\nGood morning. Thank you for j...</td>\n",
       "      <td>4237</td>\n",
       "      <td>27344</td>\n",
       "      <td>1531</td>\n",
       "      <td>15</td>\n",
       "      <td>john campbell good morning thank joining call ...</td>\n",
       "      <td>[(quarter, 71), (year, 59), (loan, 45), (ago, ...</td>\n",
       "      <td>[John Campbell\\n\\nGood morning., Thank you for...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.6510444283485...</td>\n",
       "      <td>198</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, ...</td>\n",
       "      <td>{1: 78, -1: 38, 0: 82}</td>\n",
       "      <td>[good, morning, thank, join, today, ceo, discu...</td>\n",
       "      <td>Wells Fargo &amp; Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs</td>\n",
       "      <td>Carey Halio\\n\\nGood morning. This is Carey Hal...</td>\n",
       "      <td>2585</td>\n",
       "      <td>16623</td>\n",
       "      <td>911</td>\n",
       "      <td>15</td>\n",
       "      <td>carey halio good morning carey halio head inve...</td>\n",
       "      <td>[(billion, 39), (year, 36), (quarter, 31), (re...</td>\n",
       "      <td>[Carey Halio\\n\\nGood morning., This is Carey H...</td>\n",
       "      <td>[{'label': 'Positive', 'score': 0.536954581737...</td>\n",
       "      <td>139</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, -...</td>\n",
       "      <td>{1: 53, -1: 28, 0: 58}</td>\n",
       "      <td>[good, morning, fourth, quarter, earning, conf...</td>\n",
       "      <td>Goldman Sachs Group Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usb</td>\n",
       "      <td>George Anderson\\n\\nThank you, Brad. Good morni...</td>\n",
       "      <td>2341</td>\n",
       "      <td>15185</td>\n",
       "      <td>846</td>\n",
       "      <td>15</td>\n",
       "      <td>george anderson thank brad good morning everyo...</td>\n",
       "      <td>[(quarter, 34), (deposit, 28), (billion, 26), ...</td>\n",
       "      <td>[George Anderson\\n\\nThank you, Brad., Good mor...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.9224075675010...</td>\n",
       "      <td>110</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, ...</td>\n",
       "      <td>{1: 34, -1: 3, 0: 73}</td>\n",
       "      <td>[thank, good, morning, today, prepared, remark...</td>\n",
       "      <td>US Bancorp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jpm</td>\n",
       "      <td>Jeremy Barnum\\n\\nThanks, and good morning, eve...</td>\n",
       "      <td>1991</td>\n",
       "      <td>12535</td>\n",
       "      <td>702</td>\n",
       "      <td>5</td>\n",
       "      <td>jeremy barnum thanks good morning everyone pre...</td>\n",
       "      <td>[(year, 72), (billion, 40), (quarter, 38), (dr...</td>\n",
       "      <td>[Jeremy Barnum\\n\\nThanks, and good morning, ev...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.8495514988899...</td>\n",
       "      <td>112</td>\n",
       "      <td>[0, 0, 0, 0, -1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1...</td>\n",
       "      <td>[0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1...</td>\n",
       "      <td>{1: 42, -1: 24, 0: 46}</td>\n",
       "      <td>[thank, good, morning, presentation, available...</td>\n",
       "      <td>JPMorgan Chase &amp; Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cof</td>\n",
       "      <td>Jeff Norris\\n\\nThanks very much, Amy, and welc...</td>\n",
       "      <td>2424</td>\n",
       "      <td>15581</td>\n",
       "      <td>835</td>\n",
       "      <td>35</td>\n",
       "      <td>jeff norris thanks much amy welcome everybody ...</td>\n",
       "      <td>[(quarter, 86), (year, 35), (first, 33), (basi...</td>\n",
       "      <td>[Jeff Norris\\n\\nThanks very much, Amy, and wel...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.9985010623931...</td>\n",
       "      <td>140</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1,...</td>\n",
       "      <td>{1: 52, -1: 25, 0: 63}</td>\n",
       "      <td>[thank, welcome, quarter, earning, conference,...</td>\n",
       "      <td>Capital One Financial Corp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ms</td>\n",
       "      <td>James Gorman\\n\\nGood morning, everyone and tha...</td>\n",
       "      <td>2137</td>\n",
       "      <td>13983</td>\n",
       "      <td>732</td>\n",
       "      <td>10</td>\n",
       "      <td>james gorman good morning everyone thank joini...</td>\n",
       "      <td>[(year, 31), (billion, 30), (revenue, 29), (qu...</td>\n",
       "      <td>[James Gorman\\n\\nGood morning, everyone and th...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.6943262815475...</td>\n",
       "      <td>120</td>\n",
       "      <td>[0, 1, 1, 0, 1, -1, -1, -1, 0, 1, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 2, 2, 3, 2, 1, 0, 0, 1, 1, 2, 3, 4, 5, ...</td>\n",
       "      <td>{1: 56, -1: 26, 0: 38}</td>\n",
       "      <td>[good, morning, thank, join, quarter, eventful...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfc</td>\n",
       "      <td>Ankur Vyas\\n\\nThank you, Allay, and good morni...</td>\n",
       "      <td>4445</td>\n",
       "      <td>28748</td>\n",
       "      <td>1621</td>\n",
       "      <td>41</td>\n",
       "      <td>ankur vyas thank allay good morning everyone w...</td>\n",
       "      <td>[(truist, 41), (quarter, 41), (client, 34), (d...</td>\n",
       "      <td>[Ankur Vyas\\n\\nThank you, Allay, and good morn...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.9455332159996...</td>\n",
       "      <td>196</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 2,...</td>\n",
       "      <td>{1: 90, -1: 21, 0: 85}</td>\n",
       "      <td>[thank, good, morning, quarter, earning, today...</td>\n",
       "      <td>Truist Financial Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>td</td>\n",
       "      <td>Brooke Hales\\n\\nThank you, Operator. Good afte...</td>\n",
       "      <td>3973</td>\n",
       "      <td>26737</td>\n",
       "      <td>1266</td>\n",
       "      <td>31</td>\n",
       "      <td>brooke hale thank operator good afternoon welc...</td>\n",
       "      <td>[(quarter, 57), (year, 54), (bank, 44), (td, 3...</td>\n",
       "      <td>[Brooke Hales\\n\\nThank you, Operator., Good af...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.9762963652610...</td>\n",
       "      <td>203</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>{1: 95, -1: 25, 0: 83}</td>\n",
       "      <td>[thank, good, afternoon, welcome, quarter, inv...</td>\n",
       "      <td>Toronto-Dominion Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c</td>\n",
       "      <td>Jen Landis\\n\\nThank you, operator. Good mornin...</td>\n",
       "      <td>4797</td>\n",
       "      <td>29936</td>\n",
       "      <td>1849</td>\n",
       "      <td>24</td>\n",
       "      <td>jen landis thank operator good morning thank j...</td>\n",
       "      <td>[(client, 45), (revenue, 38), (quarter, 35), (...</td>\n",
       "      <td>[Jen Landis\\n\\nThank you, operator., Good morn...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.9504846930503...</td>\n",
       "      <td>236</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 2, 3, 4, 5, 6, 6, 6, 7, 8, ...</td>\n",
       "      <td>{1: 115, -1: 27, 0: 94}</td>\n",
       "      <td>[thank, operator, good, morning, thank, join, ...</td>\n",
       "      <td>Citigroup Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bac</td>\n",
       "      <td>Lee McEntire\\n\\nThank you, Catherine. Good mor...</td>\n",
       "      <td>6242</td>\n",
       "      <td>38407</td>\n",
       "      <td>2393</td>\n",
       "      <td>45</td>\n",
       "      <td>lee mcentire thank catherine good morning welc...</td>\n",
       "      <td>[(quarter, 98), (deposit, 61), (billion, 60), ...</td>\n",
       "      <td>[Lee McEntire\\n\\nThank you, Catherine., Good m...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.9740446805953...</td>\n",
       "      <td>309</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>{1: 128, -1: 32, 0: 149}</td>\n",
       "      <td>[thank, good, morning, thank, join, review, qu...</td>\n",
       "      <td>Bank of America Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pnc</td>\n",
       "      <td>William Demchak\\n\\nThank you, Bryan, and good ...</td>\n",
       "      <td>3109</td>\n",
       "      <td>19430</td>\n",
       "      <td>1165</td>\n",
       "      <td>28</td>\n",
       "      <td>william demchak thank bryan good morning every...</td>\n",
       "      <td>[(quarter, 44), (deposit, 37), (billion, 28), ...</td>\n",
       "      <td>[William Demchak\\n\\nThank you, Bryan, and good...</td>\n",
       "      <td>[{'label': 'Neutral', 'score': 0.7638133764266...</td>\n",
       "      <td>153</td>\n",
       "      <td>[0, 1, 1, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1,...</td>\n",
       "      <td>[0, 1, 2, 2, 3, 3, 4, 4, 5, 4, 5, 5, 5, 5, 6, ...</td>\n",
       "      <td>{1: 43, -1: 15, 0: 95}</td>\n",
       "      <td>[thank, good, morning, slide, quarterly, resul...</td>\n",
       "      <td>PNC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company                                         transcript  word_count   \n",
       "0      wfc  John Campbell\\n\\nGood morning. Thank you for j...        4237  \\\n",
       "1       gs  Carey Halio\\n\\nGood morning. This is Carey Hal...        2585   \n",
       "2      usb  George Anderson\\n\\nThank you, Brad. Good morni...        2341   \n",
       "3      jpm  Jeremy Barnum\\n\\nThanks, and good morning, eve...        1991   \n",
       "4      cof  Jeff Norris\\n\\nThanks very much, Amy, and welc...        2424   \n",
       "5       ms  James Gorman\\n\\nGood morning, everyone and tha...        2137   \n",
       "6      tfc  Ankur Vyas\\n\\nThank you, Allay, and good morni...        4445   \n",
       "7       td  Brooke Hales\\n\\nThank you, Operator. Good afte...        3973   \n",
       "8        c  Jen Landis\\n\\nThank you, operator. Good mornin...        4797   \n",
       "9      bac  Lee McEntire\\n\\nThank you, Catherine. Good mor...        6242   \n",
       "10     pnc  William Demchak\\n\\nThank you, Bryan, and good ...        3109   \n",
       "\n",
       "    char_count  stopwords  numerics   \n",
       "0        27344       1531        15  \\\n",
       "1        16623        911        15   \n",
       "2        15185        846        15   \n",
       "3        12535        702         5   \n",
       "4        15581        835        35   \n",
       "5        13983        732        10   \n",
       "6        28748       1621        41   \n",
       "7        26737       1266        31   \n",
       "8        29936       1849        24   \n",
       "9        38407       2393        45   \n",
       "10       19430       1165        28   \n",
       "\n",
       "                                         p_transcript   \n",
       "0   john campbell good morning thank joining call ...  \\\n",
       "1   carey halio good morning carey halio head inve...   \n",
       "2   george anderson thank brad good morning everyo...   \n",
       "3   jeremy barnum thanks good morning everyone pre...   \n",
       "4   jeff norris thanks much amy welcome everybody ...   \n",
       "5   james gorman good morning everyone thank joini...   \n",
       "6   ankur vyas thank allay good morning everyone w...   \n",
       "7   brooke hale thank operator good afternoon welc...   \n",
       "8   jen landis thank operator good morning thank j...   \n",
       "9   lee mcentire thank catherine good morning welc...   \n",
       "10  william demchak thank bryan good morning every...   \n",
       "\n",
       "                                         top_10_words   \n",
       "0   [(quarter, 71), (year, 59), (loan, 45), (ago, ...  \\\n",
       "1   [(billion, 39), (year, 36), (quarter, 31), (re...   \n",
       "2   [(quarter, 34), (deposit, 28), (billion, 26), ...   \n",
       "3   [(year, 72), (billion, 40), (quarter, 38), (dr...   \n",
       "4   [(quarter, 86), (year, 35), (first, 33), (basi...   \n",
       "5   [(year, 31), (billion, 30), (revenue, 29), (qu...   \n",
       "6   [(truist, 41), (quarter, 41), (client, 34), (d...   \n",
       "7   [(quarter, 57), (year, 54), (bank, 44), (td, 3...   \n",
       "8   [(client, 45), (revenue, 38), (quarter, 35), (...   \n",
       "9   [(quarter, 98), (deposit, 61), (billion, 60), ...   \n",
       "10  [(quarter, 44), (deposit, 37), (billion, 28), ...   \n",
       "\n",
       "                              tokenized_raw_sentences   \n",
       "0   [John Campbell\\n\\nGood morning., Thank you for...  \\\n",
       "1   [Carey Halio\\n\\nGood morning., This is Carey H...   \n",
       "2   [George Anderson\\n\\nThank you, Brad., Good mor...   \n",
       "3   [Jeremy Barnum\\n\\nThanks, and good morning, ev...   \n",
       "4   [Jeff Norris\\n\\nThanks very much, Amy, and wel...   \n",
       "5   [James Gorman\\n\\nGood morning, everyone and th...   \n",
       "6   [Ankur Vyas\\n\\nThank you, Allay, and good morn...   \n",
       "7   [Brooke Hales\\n\\nThank you, Operator., Good af...   \n",
       "8   [Jen Landis\\n\\nThank you, operator., Good morn...   \n",
       "9   [Lee McEntire\\n\\nThank you, Catherine., Good m...   \n",
       "10  [William Demchak\\n\\nThank you, Bryan, and good...   \n",
       "\n",
       "                                       finbert_scores   \n",
       "0   [{'label': 'Neutral', 'score': 0.6510444283485...  \\\n",
       "1   [{'label': 'Positive', 'score': 0.536954581737...   \n",
       "2   [{'label': 'Neutral', 'score': 0.9224075675010...   \n",
       "3   [{'label': 'Neutral', 'score': 0.8495514988899...   \n",
       "4   [{'label': 'Neutral', 'score': 0.9985010623931...   \n",
       "5   [{'label': 'Neutral', 'score': 0.6943262815475...   \n",
       "6   [{'label': 'Neutral', 'score': 0.9455332159996...   \n",
       "7   [{'label': 'Neutral', 'score': 0.9762963652610...   \n",
       "8   [{'label': 'Neutral', 'score': 0.9504846930503...   \n",
       "9   [{'label': 'Neutral', 'score': 0.9740446805953...   \n",
       "10  [{'label': 'Neutral', 'score': 0.7638133764266...   \n",
       "\n",
       "    number_tokenized_raw_sentences   \n",
       "0                              198  \\\n",
       "1                              139   \n",
       "2                              110   \n",
       "3                              112   \n",
       "4                              140   \n",
       "5                              120   \n",
       "6                              196   \n",
       "7                              203   \n",
       "8                              236   \n",
       "9                              309   \n",
       "10                             153   \n",
       "\n",
       "                                  finbert_score_array   \n",
       "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...  \\\n",
       "1   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -...   \n",
       "2   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...   \n",
       "3   [0, 0, 0, 0, -1, 0, 0, 0, 1, -1, 0, 0, 0, 0, 1...   \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0,...   \n",
       "5   [0, 1, 1, 0, 1, -1, -1, -1, 0, 1, 0, 1, 1, 1, ...   \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1,...   \n",
       "7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8   [0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, ...   \n",
       "9   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10  [0, 1, 1, 0, 1, 0, 1, 0, 1, -1, 1, 0, 0, 0, 1,...   \n",
       "\n",
       "                       finbert_score_array_cumulative   \n",
       "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, ...  \\\n",
       "1   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, -...   \n",
       "2   [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, ...   \n",
       "3   [0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1...   \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1,...   \n",
       "5   [0, 1, 2, 2, 3, 2, 1, 0, 0, 1, 1, 2, 3, 4, 5, ...   \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 2,...   \n",
       "7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8   [0, 1, 1, 1, 1, 1, 2, 3, 4, 5, 6, 6, 6, 7, 8, ...   \n",
       "9   [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "10  [0, 1, 2, 2, 3, 3, 4, 4, 5, 4, 5, 5, 5, 5, 6, ...   \n",
       "\n",
       "            sentiment_totals   \n",
       "0     {1: 78, -1: 38, 0: 82}  \\\n",
       "1     {1: 53, -1: 28, 0: 58}   \n",
       "2      {1: 34, -1: 3, 0: 73}   \n",
       "3     {1: 42, -1: 24, 0: 46}   \n",
       "4     {1: 52, -1: 25, 0: 63}   \n",
       "5     {1: 56, -1: 26, 0: 38}   \n",
       "6     {1: 90, -1: 21, 0: 85}   \n",
       "7     {1: 95, -1: 25, 0: 83}   \n",
       "8    {1: 115, -1: 27, 0: 94}   \n",
       "9   {1: 128, -1: 32, 0: 149}   \n",
       "10    {1: 43, -1: 15, 0: 95}   \n",
       "\n",
       "                             processed_corpus_for_lda   \n",
       "0   [good, morning, thank, join, today, ceo, discu...  \\\n",
       "1   [good, morning, fourth, quarter, earning, conf...   \n",
       "2   [thank, good, morning, today, prepared, remark...   \n",
       "3   [thank, good, morning, presentation, available...   \n",
       "4   [thank, welcome, quarter, earning, conference,...   \n",
       "5   [good, morning, thank, join, quarter, eventful...   \n",
       "6   [thank, good, morning, quarter, earning, today...   \n",
       "7   [thank, good, afternoon, welcome, quarter, inv...   \n",
       "8   [thank, operator, good, morning, thank, join, ...   \n",
       "9   [thank, good, morning, thank, join, review, qu...   \n",
       "10  [thank, good, morning, slide, quarterly, resul...   \n",
       "\n",
       "              full_company_name  \n",
       "0              Wells Fargo & Co  \n",
       "1       Goldman Sachs Group Inc  \n",
       "2                    US Bancorp  \n",
       "3           JPMorgan Chase & Co  \n",
       "4   Capital One Financial Corp.  \n",
       "5                Morgan Stanley  \n",
       "6         Truist Financial Corp  \n",
       "7         Toronto-Dominion Bank  \n",
       "8                 Citigroup Inc  \n",
       "9          Bank of America Corp  \n",
       "10                          PNC  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1155d7-6739-4f2d-9115-816a116eab8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Finding top 10 terms which are present in all earnings calls transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7531ca-3cc8-4f58-bb8e-a650aa0de404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_top_10_terms = []\n",
    "for i, c_d in enumerate(list(df['top_10_words'])):\n",
    "    all_top_10_terms.extend([t[0] for t in c_d])\n",
    "all_top_10_terms = list(set(all_top_10_terms))\n",
    "# all_top_10_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44aa82a2-1af0-4649-ac5a-2c8d1540a7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_count_all_top_10_terms = Counter()  \n",
    "for term in all_top_10_terms:\n",
    "    count_for_term = 0\n",
    "    for doc_id in list(df['top_10_words']):\n",
    "        if term in [t[0] for t in doc_id]:\n",
    "            count_for_term += 1\n",
    "    total_count_all_top_10_terms[term] = count_for_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdea9c09-3a16-41c6-9764-2fa5de3dd771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quarter', 11),\n",
       " ('year', 9),\n",
       " ('deposit', 7),\n",
       " ('billion', 7),\n",
       " ('client', 5),\n",
       " ('first', 4),\n",
       " ('revenue', 4),\n",
       " ('business', 4),\n",
       " ('loan', 3),\n",
       " ('rate', 3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count_all_top_10_terms.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c5de1-5786-4c20-93c2-963a94a551f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Applying TF-IDF to Earnings Calls Transcripts corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c79b1981-4e0c-4d0b-8a4f-e9f535ea9fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = list(df['p_transcript'])\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec853d31-b3f3-4840-8161-2d1898487f19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d69ed329-964a-4c37-92db-f3d6994f6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v=TfidfVectorizer(ngram_range=(1,3))\n",
    "X=tfidf_v.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e48c363-97c8-4814-8356-5c0edfe82761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 38972)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "018c3baa-a491-44a4-8f50-b3228fa93d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abated', 'abated cet', 'abated cet ratio', 'abated period',\n",
       "       'abated period end', 'ability', 'ability attract',\n",
       "       'ability attract asset', 'ability complete',\n",
       "       'ability complete work', 'ability continue',\n",
       "       'ability continue deliver', 'ability convert',\n",
       "       'ability convert new', 'ability realize',\n",
       "       'ability realize significant', 'able', 'able discus',\n",
       "       'able discus specific', 'absence', 'absence goodwill',\n",
       "       'absence goodwill impairment', 'absent', 'absent geopolitical',\n",
       "       'absent geopolitical surprise', 'absent retailer',\n",
       "       'absent retailer partner', 'absolute', 'absolute level',\n",
       "       'absolute level interest', 'absolutely', 'absolutely paramount',\n",
       "       'absolutely paramount mark', 'absorb', 'absorb temporary',\n",
       "       'absorb temporary upfront', 'absorbed', 'absorbed partner',\n",
       "       'absorbed partner impact', 'absorbing'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v.get_feature_names_out()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ddb7511-d5e7-439c-814e-b125ca977992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 3),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7738f86-dab8-4963-b3d3-6b0501dcbd86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abated</th>\n",
       "      <th>abated cet</th>\n",
       "      <th>abated cet ratio</th>\n",
       "      <th>abated period</th>\n",
       "      <th>abated period end</th>\n",
       "      <th>ability</th>\n",
       "      <th>ability attract</th>\n",
       "      <th>ability attract asset</th>\n",
       "      <th>ability complete</th>\n",
       "      <th>ability complete work</th>\n",
       "      <th>...</th>\n",
       "      <th>zelle grew</th>\n",
       "      <th>zelle grew past</th>\n",
       "      <th>zelle interaction</th>\n",
       "      <th>zelle interaction continue</th>\n",
       "      <th>zelle particular</th>\n",
       "      <th>zelle particular underscoring</th>\n",
       "      <th>zelle remember</th>\n",
       "      <th>zelle remember back</th>\n",
       "      <th>zelle transaction</th>\n",
       "      <th>zelle transaction crossed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.012315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.014953</td>\n",
       "      <td>0.014953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.007765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows  38972 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abated  abated cet  abated cet ratio  abated period  abated period end   \n",
       "0   0.018331    0.009165          0.009165       0.009165           0.009165  \\\n",
       "1   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "2   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "3   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "4   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "5   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "6   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "7   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "8   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "9   0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "10  0.000000    0.000000          0.000000       0.000000           0.000000   \n",
       "\n",
       "     ability  ability attract  ability attract asset  ability complete   \n",
       "0   0.012315         0.000000               0.000000          0.009165  \\\n",
       "1   0.000000         0.000000               0.000000          0.000000   \n",
       "2   0.009068         0.000000               0.000000          0.000000   \n",
       "3   0.000000         0.000000               0.000000          0.000000   \n",
       "4   0.000000         0.000000               0.000000          0.000000   \n",
       "5   0.010045         0.014953               0.014953          0.000000   \n",
       "6   0.006300         0.000000               0.000000          0.000000   \n",
       "7   0.000000         0.000000               0.000000          0.000000   \n",
       "8   0.000000         0.000000               0.000000          0.000000   \n",
       "9   0.000000         0.000000               0.000000          0.000000   \n",
       "10  0.000000         0.000000               0.000000          0.000000   \n",
       "\n",
       "    ability complete work  ...  zelle grew  zelle grew past   \n",
       "0                0.009165  ...    0.000000         0.000000  \\\n",
       "1                0.000000  ...    0.000000         0.000000   \n",
       "2                0.000000  ...    0.000000         0.000000   \n",
       "3                0.000000  ...    0.000000         0.000000   \n",
       "4                0.000000  ...    0.000000         0.000000   \n",
       "5                0.000000  ...    0.000000         0.000000   \n",
       "6                0.000000  ...    0.000000         0.000000   \n",
       "7                0.000000  ...    0.000000         0.000000   \n",
       "8                0.000000  ...    0.000000         0.000000   \n",
       "9                0.000000  ...    0.007765         0.007765   \n",
       "10               0.000000  ...    0.000000         0.000000   \n",
       "\n",
       "    zelle interaction  zelle interaction continue  zelle particular   \n",
       "0            0.000000                    0.000000          0.000000  \\\n",
       "1            0.000000                    0.000000          0.000000   \n",
       "2            0.000000                    0.000000          0.000000   \n",
       "3            0.000000                    0.000000          0.000000   \n",
       "4            0.000000                    0.000000          0.000000   \n",
       "5            0.000000                    0.000000          0.000000   \n",
       "6            0.000000                    0.000000          0.009378   \n",
       "7            0.000000                    0.000000          0.000000   \n",
       "8            0.000000                    0.000000          0.000000   \n",
       "9            0.007765                    0.007765          0.000000   \n",
       "10           0.000000                    0.000000          0.000000   \n",
       "\n",
       "    zelle particular underscoring  zelle remember  zelle remember back   \n",
       "0                        0.000000        0.000000             0.000000  \\\n",
       "1                        0.000000        0.000000             0.000000   \n",
       "2                        0.000000        0.000000             0.000000   \n",
       "3                        0.000000        0.000000             0.000000   \n",
       "4                        0.000000        0.000000             0.000000   \n",
       "5                        0.000000        0.000000             0.000000   \n",
       "6                        0.009378        0.000000             0.000000   \n",
       "7                        0.000000        0.000000             0.000000   \n",
       "8                        0.000000        0.000000             0.000000   \n",
       "9                        0.000000        0.007765             0.007765   \n",
       "10                       0.000000        0.000000             0.000000   \n",
       "\n",
       "    zelle transaction  zelle transaction crossed  \n",
       "0            0.000000                   0.000000  \n",
       "1            0.000000                   0.000000  \n",
       "2            0.000000                   0.000000  \n",
       "3            0.000000                   0.000000  \n",
       "4            0.000000                   0.000000  \n",
       "5            0.000000                   0.000000  \n",
       "6            0.000000                   0.000000  \n",
       "7            0.000000                   0.000000  \n",
       "8            0.000000                   0.000000  \n",
       "9            0.007765                   0.007765  \n",
       "10           0.000000                   0.000000  \n",
       "\n",
       "[11 rows x 38972 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(X, columns=tfidf_v.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032f4e1-9b6f-4f9f-b61d-cfda0806096e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Finding top 10 terms by maximum TF-IDF score across corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfa52832-0174-44eb-83f4-e04f737c0420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_10_tf_idf_values = []\n",
    "for i in range(0,11):\n",
    "    top_10_tf_idf_values.append(tfidf_df.iloc[i].nlargest(10).to_frame().T.to_dict())\n",
    "\n",
    "# top_10_tf_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3543e256-de1c-4108-a152-d9bdbdbda53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_top_10_tfidf_terms = []\n",
    "for i, c_d in enumerate(top_10_tf_idf_values):\n",
    "    all_top_10_tfidf_terms.extend(list(c_d.keys()))\n",
    "\n",
    "all_top_10_tfidf_terms = list(set(all_top_10_tfidf_terms))\n",
    "# all_top_10_tfidf_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63107605-aaab-42f5-ac82-c76380dcabba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_count_c = Counter()  \n",
    "for tfidf_term in all_top_10_tfidf_terms:\n",
    "    count_for_term = 0\n",
    "    for doc_id in range(0,11):\n",
    "        if tfidf_term in list(top_10_tf_idf_values[doc_id].keys()):\n",
    "            count_for_term += 1\n",
    "    total_count_c[tfidf_term] = count_for_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f920a5c8-090a-40f9-8159-dbe827e74fed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quarter', 11),\n",
       " ('year', 8),\n",
       " ('billion', 6),\n",
       " ('deposit', 5),\n",
       " ('revenue', 4),\n",
       " ('client', 4),\n",
       " ('first', 3),\n",
       " ('adjusted', 3),\n",
       " ('slide', 3),\n",
       " ('first quarter', 3)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count_c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7deb4e-6377-4c2f-96d7-fec7ad690bfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Applying LDA to conduct Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b46317d-0b26-4de0-b8a5-a2c8fb256488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpora' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m NUM_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ID2word \u001b[38;5;241m=\u001b[39m \u001b[43mcorpora\u001b[49m\u001b[38;5;241m.\u001b[39mDictionary(lda_corpus_list)\n\u001b[1;32m      3\u001b[0m train_corpus_bow \u001b[38;5;241m=\u001b[39m [ID2word\u001b[38;5;241m.\u001b[39mdoc2bow(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m lda_corpus_list]\n\u001b[1;32m      5\u001b[0m TFIDF \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mTfidfModel(train_corpus_bow)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpora' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_topics = 5\n",
    "ID2word = corpora.Dictionary(lda_corpus_list)\n",
    "train_corpus_bow = [ID2word.doc2bow(doc) for doc in lda_corpus_list]\n",
    "\n",
    "TFIDF = models.TfidfModel(train_corpus_bow)\n",
    "train_corpus_tfidf = TFIDF[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103bfbb-0852-4863-9ecd-2cec24b0fecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_bow_model = gensim.models.LdaMulticore(corpus=train_corpus_bow, num_topics=NUM_topics, id2word=ID2word, passes=100)\n",
    "lda_bow_model.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c67d4-39f4-494e-a98e-f661fec9dfd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coherence_model_lda_bow = gensim.models.CoherenceModel(model=lda_bow_model, texts=lda_corpus_list, dictionary=ID2word, coherence='c_v')\n",
    "coherence_lda_bow = coherence_model_lda_bow.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a857af-39b0-4bb2-be6c-bce74d2f579e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_tfidf_model = gensim.models.LdaMulticore(corpus=train_corpus_tfidf, num_topics=NUM_topics, id2word=ID2word, passes=100)\n",
    "lda_tfidf_model.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c09e88-1309-48f4-bc1f-9543334ccbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coherence_model_lda_tfidf = gensim.models.CoherenceModel(model=lda_tfidf_model, texts=lda_corpus_list, dictionary=ID2word, coherence='c_v')\n",
    "coherence_lda_tfidf = coherence_model_lda_tfidf.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6bf689-726b-47b8-a0c0-bb85cda6c33a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions for generating data for all visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8040316-1b82-4e7e-b2d7-1cb49ae04b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_for_viz_number_of_characters():\n",
    "    company_character_count_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        company_character_count_data.append({\"company\": row['full_company_name'], \"number_of_characters\":row['char_count']})\n",
    "\n",
    "    company_character_count_data = sorted(company_character_count_data, key=lambda d: d['number_of_characters']) \n",
    "    return company_character_count_data\n",
    "\n",
    "def get_data_for_viz_number_of_sentences():\n",
    "    company_sentence_count_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        company_sentence_count_data.append({\"company\": row['full_company_name'], \"number_of_sentences\":row['number_tokenized_raw_sentences']})\n",
    "\n",
    "    company_sentence_count_data = sorted(company_sentence_count_data, key=lambda d: d['number_of_sentences']) \n",
    "    return company_sentence_count_data\n",
    "\n",
    "def get_data_for_viz_number_of_words():\n",
    "    maximum_number_words = max(list(df['word_count']))\n",
    "    word_types = ['Words', 'Stop Words']\n",
    "    \n",
    "    grouped_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        grouped_data.append(\n",
    "            {\n",
    "                \"company\": row['full_company_name'], \n",
    "                \"words\": [\n",
    "                    {\"word\": \"Words\", \"count\": row['word_count']}, \n",
    "                    {\"word\": \"Stop Words\", \"count\": row['stopwords']}\n",
    "                ]\n",
    "            })\n",
    "    \n",
    "    grouped_data = sorted(grouped_data, key=lambda d: d['words'][0]['count']) \n",
    "    company_names = [i['company'] for i in grouped_data]\n",
    "    return company_names, maximum_number_words, word_types, grouped_data\n",
    "\n",
    "def get_data_for_viz_prefix_sum_sentiments():\n",
    "    company_prefix_sum_data = []\n",
    "    grid = []\n",
    "    n_rows = 6\n",
    "    n_cols = 2\n",
    "            \n",
    "    for i in range(0, n_rows):\n",
    "        for j in range(0, n_cols):\n",
    "            grid.append([i, 0 if not j else 1])\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        company_prefix_sum_data.append(\n",
    "            {\n",
    "                \"company\": row['full_company_name'],\n",
    "                \"prefix_sum_sentiments\": [\n",
    "                    {\"number\": index, \"prefix_sum\": i} for index, i in enumerate(row['finbert_score_array_cumulative'])\n",
    "                ],\n",
    "                \"row\": grid[index][0],\n",
    "                \"col\": grid[index][1]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    return company_prefix_sum_data\n",
    "\n",
    "def get_data_for_viz_top_10_words_by_frequency():\n",
    "    top_10_words_by_frequency = []\n",
    "    for row in total_count_all_top_10_terms.most_common(10):\n",
    "        top_10_words_by_frequency.append({\"word\": row[0], \"number_of_transcripts\":row[1]})\n",
    "    return top_10_words_by_frequency\n",
    "\n",
    "def get_data_for_viz_top_10_words_by_tfidf():\n",
    "    top_10_words_by_tfidf = []\n",
    "    for row in total_count_c.most_common(10):\n",
    "        top_10_words_by_tfidf.append({\"word\": row[0], \"number_of_transcripts\":row[1]})\n",
    "    return top_10_words_by_tfidf\n",
    "\n",
    "def get_data_for_viz_sentiment_counts():\n",
    "    sentiment_counts = []\n",
    "    for index, row in df.iterrows():\n",
    "        pct_of_positive = float(row['sentiment_totals'][1])/(row['sentiment_totals'][1]+row['sentiment_totals'][-1]+row['sentiment_totals'][0])\n",
    "        sentiment_counts.append(\n",
    "            {\n",
    "                \"company\": row['full_company_name'], \n",
    "                \"sentiment_count\": [\n",
    "                    {\"sentiment\": \"Positive\", \"count\":row['sentiment_totals'][1]}, \n",
    "                    {\"sentiment\": \"Negative\", \"count\":row['sentiment_totals'][-1]}, \n",
    "                    {\"sentiment\": \"Neutral\", \"count\":row['sentiment_totals'][0]}\n",
    "                ],\n",
    "                \"pct_of_positive\": pct_of_positive\n",
    "            }\n",
    "        )\n",
    "    sentiment_counts = sorted(sentiment_counts, key=lambda d: d['pct_of_positive'])\n",
    "    company_names = [i['company'] for i in sentiment_counts]\n",
    "    return sentiment_counts, company_names\n",
    "\n",
    "def get_data_for_parts_of_transcript_boxplot():\n",
    "    box_plot_data = []\n",
    "    box_plot_numbers = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        company_name = row['full_company_name']\n",
    "        parts_of_transcript = np.array_split(row['finbert_score_array'], 4)\n",
    "        \n",
    "        box_plot_data.append({\"company\": company_name, \"part_of_transcript\": \"1st 5 sentences\", \"s_value\": np.mean(row['finbert_score_array'][:5])})\n",
    "        box_plot_data.append({\"company\": company_name, \"part_of_transcript\": \"Last 5 sentences\", \"s_value\": np.mean(row['finbert_score_array'][-5:])})\n",
    "        box_plot_data.append({\"company\": company_name, \"part_of_transcript\": \"1st quarter of transcript\", \"s_value\": np.mean(parts_of_transcript[0])})\n",
    "        box_plot_data.append({\"company\": company_name, \"part_of_transcript\": \"2nd quarter of transcript\", \"s_value\": np.mean(parts_of_transcript[1])})\n",
    "        box_plot_data.append({\"company\": company_name, \"part_of_transcript\": \"3rd quarter of transcript\", \"s_value\": np.mean(parts_of_transcript[2])})\n",
    "        box_plot_data.append({\"company\": company_name, \"part_of_transcript\": \"4th quarter of transcript\", \"s_value\": np.mean(parts_of_transcript[3])})\n",
    "        \n",
    "    df_stat = pd.DataFrame(box_plot_data)\n",
    "    df_stat_group = df_stat.groupby(['part_of_transcript'])\n",
    "    \n",
    "    \n",
    "    for group_id in ['1st 5 sentences', '1st quarter of transcript', '2nd quarter of transcript', '3rd quarter of transcript', '4th quarter of transcript', 'Last 5 sentences']:\n",
    "        temp_list = list(df_stat_group.get_group(group_id)['s_value'])\n",
    "        temp_list.sort()\n",
    "        q75, q25 = np.percentile(temp_list, [75 ,25])\n",
    "        box_plot_numbers.append({\"key\": group_id, \"value\": {\n",
    "            \"q1\": q25, \n",
    "            \"q3\": q75, \n",
    "            \"interQuantileRange\": q75 - q25, \n",
    "            \"median\": np.median(temp_list), \n",
    "            \"min\": min(temp_list),\n",
    "            \"max\": max(temp_list)\n",
    "        }\n",
    "    })    \n",
    "    \n",
    "    return box_plot_data, box_plot_numbers\n",
    "\n",
    "def get_data_for_viz_lda_topics_matrix():\n",
    "    data = []\n",
    "    topics = set()\n",
    "    words = set()\n",
    "    max_value = -1\n",
    "\n",
    "    for i in lda_bow_model.print_topics(num_words=5):\n",
    "        tn = re.sub('[^a-zA-Z0-9.]', ' ', i[1])\n",
    "        tnl = tn.split()\n",
    "        res = [[tnl[i], tnl[i + 1]] for i in range(0, len(tnl), 2)]\n",
    "\n",
    "        for index, topic_word in enumerate(res):\n",
    "            topics.add(f\"Topic {i[0]+1}\")\n",
    "            words.add(f\"Word {index+1}\")\n",
    "            if max_value < float(topic_word[0]):\n",
    "                max_value = float(topic_word[0])\n",
    "            data.append({\"topic\": f\"Topic {i[0]+1}\", \"word\": f\"Word {index+1}\", \"value\": topic_word[1], \"significance_score\": float(topic_word[0])})\n",
    "\n",
    "    topics = sorted(list(topics))\n",
    "    words = sorted(list(words))\n",
    "    \n",
    "    return data, topics, words, max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a70250-ae4d-4e2b-b94f-ff7aa517732f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_parts_of_transcript_boxplot = get_data_for_parts_of_transcript_boxplot()\n",
    "# data_for_parts_of_transcript_boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22bd6d-ee21-499c-8262-adf924e353c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_viz_sentiment_counts = get_data_for_viz_sentiment_counts()\n",
    "# data_for_viz_sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af028251-af7f-4214-8788-fabc6b138ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_viz_top_10_words_by_tfidf = get_data_for_viz_top_10_words_by_tfidf()\n",
    "# data_for_viz_top_10_words_by_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17abad9-dbf8-4e20-a706-2df579095937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_viz_top_10_words_by_frequency = get_data_for_viz_top_10_words_by_frequency()\n",
    "# data_for_viz_top_10_words_by_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2dba9-69eb-4dc9-aa0d-1b78c71e1f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_viz_prefix_sum_sentiments = get_data_for_viz_prefix_sum_sentiments()\n",
    "# data_for_viz_prefix_sum_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a273f-591f-40c9-8730-87f3ed7bb17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_viz_number_of_words = get_data_for_viz_number_of_words()\n",
    "# data_for_viz_number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3033512-81a5-4490-ad8f-8cfdfd996197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_viz_number_of_characters = get_data_for_viz_number_of_characters()\n",
    "# data_for_viz_number_of_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdd9b9-4a70-4860-b739-989bb5c705a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_viz_number_of_sentences = get_data_for_viz_number_of_sentences()\n",
    "# data_for_viz_number_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3de90-624c-49ef-84a9-16e1853b7cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_for_viz_lda_topics_matrix = get_data_for_viz_lda_topics_matrix()\n",
    "# data_for_viz_lda_topics_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39b5e9-2656-4eac-8f71-59ea35d5e275",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generating all visualizations using D3JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed1f9c-a4fd-49ca-a8c0-faf8585e5acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_vis_simple_bar_charts], \n",
    "    main_function=\"viz_number_of_sentences\",\n",
    "    data_dict={\"viz_data\": data_for_viz_number_of_sentences}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e79f78-3348-407b-ad69-f5165f6b191a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_vis_simple_bar_charts], \n",
    "    main_function=\"viz_number_of_characters\",\n",
    "    data_dict={\"viz_data\": data_for_viz_number_of_characters}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df4fbb-03e6-44f7-bb48-8b5c8f3c8364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_vis_grouped_bar_chart], \n",
    "    main_function=\"viz_number_of_words\",\n",
    "    data_dict={\n",
    "        \"company_names\": data_for_viz_number_of_words[0], \n",
    "        \"maximum_number_words\": data_for_viz_number_of_words[1], \n",
    "        \"word_types\": data_for_viz_number_of_words[2], \n",
    "        \"grouped_data\": data_for_viz_number_of_words[3]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69809391-d7d2-4cee-bf05-62a0544da177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_vis_small_multiple_area_chart], \n",
    "    main_function=\"viz_prefix_sum_sentiments\",\n",
    "    data_dict={\n",
    "        \"data_for_viz_prefix_sum_sentiments\": data_for_viz_prefix_sum_sentiments\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e8960-0b90-4459-a8f5-b1ddfe6e670f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_vis_bubble_chart], \n",
    "    main_function=\"viz_top_10_words\",\n",
    "    data_dict={\n",
    "        \"data_for_viz_top_10_words\": data_for_viz_top_10_words_by_frequency,\n",
    "        \"mode\": \"freq\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe3f69-878d-492e-bcbf-1fd948aa20be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_vis_bubble_chart], \n",
    "    main_function=\"viz_top_10_words\",\n",
    "    data_dict={\n",
    "        \"data_for_viz_top_10_words\": data_for_viz_top_10_words_by_tfidf,\n",
    "        \"mode\": \"tfidf\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a750d2-76e2-4399-8636-932ce42105be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_vis_pie_charts], \n",
    "    main_function=\"viz_sentiment_counts\",\n",
    "    data_dict={\n",
    "        \"data_for_viz_sentiment_counts\": data_for_viz_sentiment_counts[0],\n",
    "        \"company_name\": data_for_viz_sentiment_counts[1]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a5948-713b-4200-8faf-f8551e8b2eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_vis_box_plot], \n",
    "    main_function=\"viz_parts_of_transcript_boxplot\",\n",
    "    data_dict={\n",
    "        \"data_for_parts_of_transcript_boxplot\": data_for_parts_of_transcript_boxplot[0],\n",
    "        \"data_for_parts_of_transcript_boxplot_specifics\": data_for_parts_of_transcript_boxplot[1],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ccebc-8c70-494b-9f1c-ac850af319fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_js(\n",
    "    library_list=[d3_path, code_viz_lda_topics_matrix], \n",
    "    main_function=\"viz_lda_topics\",\n",
    "    data_dict={\n",
    "        \"data\": data_for_viz_lda_topics_matrix[0],\n",
    "        \"topics\": data_for_viz_lda_topics_matrix[1],\n",
    "        \"words\": data_for_viz_lda_topics_matrix[2],\n",
    "        \"max_value\": data_for_viz_lda_topics_matrix[3]\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
